{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root directory for the dataset\n",
    "root = \"data/DressCode/\"\n",
    "\n",
    "# Map labels to their corresponding directories\n",
    "DIRECTORY_MAP = [\"upper_body\", \"lower_body\", \"dresses\"]\n",
    "\n",
    "CLASS_MAP = [0, 0, 0, 0, 0, 1, 1, 1, 2, 2, 2, 2]\n",
    "\n",
    "CLASS_TO_NAME = [\"short sleeve top\",\n",
    "\"long sleeve top\",\n",
    "\"short sleeve outwear\"\n",
    "\"long sleeve outwear\",\n",
    "\"vest\",\n",
    "\"sling\",\n",
    "\"shorts\",\n",
    "\"trousers\",\n",
    "\"skirt\",\n",
    "\"short sleeve dress\",\n",
    "\"long sleeve dress\",\n",
    "\"vest dress\",\n",
    "\"sling dress\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the dataset\n",
    "pairs = pd.read_csv(\n",
    "    os.path.join(root, \"train_pairs_cropped.txt\"),\n",
    "    delimiter=\"\\t\",\n",
    "    header=None,\n",
    "    names=[\"model\", \"garment\", \"label\"],\n",
    ")\n",
    "\n",
    "pairs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Set the device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the encoder network\n",
    "encoder = models.resnet50()\n",
    "\n",
    "# Load the weights\n",
    "encoder.load_state_dict(torch.load(\"models/ResNet50 Cosine Similarity Loss Margin 0.2/checkpoint-6.pt\"))\n",
    "\n",
    "# Send the model to the device\n",
    "encoder = encoder.to(device)\n",
    "\n",
    "# Define the transformations for the network\n",
    "transforms = transforms.Compose([transforms.Resize((256, 192)), transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_features(image: Image) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Get the features for a given image.\n",
    "    \"\"\"\n",
    "    # Set the model to evaluation mode\n",
    "    encoder.eval()\n",
    "\n",
    "    # Resize & convert to tensor\n",
    "    image = transforms(image)\n",
    "\n",
    "    # Add a batch dimension\n",
    "    image = image.unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        return encoder(image).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUM_IMAGES x NUM_FEATURES\n",
    "features = {\"upper_body\": [], \"lower_body\": [], \"dresses\": []}\n",
    "feature_indices = {\"upper_body\": [], \"lower_body\": [], \"dresses\": []}\n",
    "\n",
    "encoder.eval()\n",
    "\n",
    "for i, (model, garment, label) in tqdm(\n",
    "    enumerate(pairs.values),\n",
    "    desc=\"Calculating Features\",\n",
    "    total=len(pairs),\n",
    "    unit=\"image\",\n",
    "):\n",
    "    # Load in the garment image\n",
    "    garment_image = Image.open(\n",
    "        os.path.join(root, DIRECTORY_MAP[label], \"cropped_images\", garment)\n",
    "    ).convert(\"RGB\")\n",
    "    \n",
    "    # Get the features\n",
    "    features[DIRECTORY_MAP[label]].append(calculate_features(garment_image))\n",
    "    feature_indices[DIRECTORY_MAP[label]].append(i)\n",
    "\n",
    "features[\"upper_body\"] = torch.cat(features[\"upper_body\"])\n",
    "features[\"lower_body\"] = torch.cat(features[\"lower_body\"])\n",
    "features[\"dresses\"] = torch.cat(features[\"dresses\"])\n",
    "\n",
    "feature_indices[\"upper_body\"] = np.array(feature_indices[\"upper_body\"])\n",
    "feature_indices[\"lower_body\"] = np.array(feature_indices[\"lower_body\"])\n",
    "feature_indices[\"dresses\"] = np.array(feature_indices[\"dresses\"])\n",
    "\n",
    "features[\"upper_body\"].shape, features[\"lower_body\"].shape, features[\"dresses\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the features\n",
    "torch.save(features, \"data/DressCode/train_features.pt\")\n",
    "torch.save(feature_indices, \"data/DressCode/train_feature_indices.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the model\n",
    "yolo = YOLO(\"models/yolov8m.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_images(image: Image, label: int,  n: int = 5) -> list[Image.Image]:\n",
    "    \"\"\"\n",
    "    Get the n most similar images to the given image.\n",
    "    \"\"\"\n",
    "\n",
    "    # Caluclate the features for the image\n",
    "    image_features = calculate_features(image)\n",
    "\n",
    "    class_features = features[DIRECTORY_MAP[label]]\n",
    "\n",
    "    # Calculate the cosine similarity for every image\n",
    "    similarities = torch.cosine_similarity(image_features, class_features)\n",
    "\n",
    "    # Find the n most similar images\n",
    "    similar_image_indices = torch.argsort(similarities, descending=True)[:n].numpy()\n",
    "\n",
    "    similar_image_indices = feature_indices[DIRECTORY_MAP[label]][similar_image_indices]\n",
    "\n",
    "    similar_images = []\n",
    "\n",
    "    for idx in similar_image_indices:\n",
    "        similar_images.append(\n",
    "            Image.open(\n",
    "                os.path.join(\n",
    "                    root, DIRECTORY_MAP[label], \"cropped_images\", pairs.iloc[idx][\"garment\"]\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return similar_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_similar_images(image: Image, similar_images: list[Image.Image]):\n",
    "    \"\"\"\n",
    "    Display the similar images.\n",
    "    \"\"\"\n",
    "\n",
    "    fig, axs = plt.subplots(1, len(similar_images) + 1, figsize=(20, 10))\n",
    "\n",
    "    # Display the anchor image\n",
    "    axs[0].imshow(image)\n",
    "    axs[0].set_title(\"Anchor Image\")\n",
    "\n",
    "    for i, similar_image in enumerate(similar_images, 1):\n",
    "        axs[i].imshow(similar_image)\n",
    "        axs[i].set_title(f\"Similar Image {i}\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(image: Image, min_confidence: float = 0.5) -> Image:\n",
    "    # Get the predictions\n",
    "    predictions = yolo.predict(image)[0]\n",
    "\n",
    "    # Get the predicted detections\n",
    "    detections = predictions.boxes\n",
    "\n",
    "    # Threshold the predictions\n",
    "    detections = detections[detections.conf > min_confidence]\n",
    "\n",
    "    for detection in detections:\n",
    "        bounding_box = detection.xyxy.cpu().numpy().squeeze()\n",
    "\n",
    "        image_cropped = image.crop(bounding_box)\n",
    "\n",
    "        image_cropped.show()\n",
    "\n",
    "        cls = detection.cls.int().item()\n",
    "        label = CLASS_MAP[cls]\n",
    "\n",
    "        similar_images = get_similar_images(image_cropped, label)\n",
    "\n",
    "        display_similar_images(image_cropped, similar_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"data\\jasper.PNG\").convert(\"RGB\")\n",
    "\n",
    "infer(image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
