{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import wandb\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Set the device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map labels to their corresponding directories\n",
    "DIRECTORY_MAP = [\"upper_body\", \"lower_body\", \"dresses\"]\n",
    "\n",
    "\n",
    "class FashionDataset(Dataset):\n",
    "    def __init__(self, root: str, pairs: str) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.transforms = transforms.Compose(\n",
    "            [transforms.Resize((256, 192)), transforms.ToTensor()]\n",
    "        )\n",
    "\n",
    "        # Root directory of the dataset\n",
    "        self.root = root\n",
    "\n",
    "        # Load in the paired data\n",
    "        self.data = pd.read_csv(\n",
    "            pairs, delimiter=\"\\t\", header=None, names=[\"model\", \"garment\", \"label\"]\n",
    "        )\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index: int) -> dict:\n",
    "        model, garment, label = self.data.iloc[index]\n",
    "\n",
    "        # Load the anchor & positive images (random choice between model and garment)\n",
    "        if random.choice([True, False]):\n",
    "            anchor = Image.open(\n",
    "                os.path.join(self.root, DIRECTORY_MAP[label], \"cropped_images\", model)\n",
    "            ).convert(\"RGB\")\n",
    "\n",
    "            positive = Image.open(\n",
    "                os.path.join(self.root, DIRECTORY_MAP[label], \"cropped_images\", garment)\n",
    "            ).convert(\"RGB\")\n",
    "        else:\n",
    "            anchor = Image.open(\n",
    "                os.path.join(self.root, DIRECTORY_MAP[label], \"cropped_images\", garment)\n",
    "            ).convert(\"RGB\")\n",
    "\n",
    "            positive = Image.open(\n",
    "                os.path.join(self.root, DIRECTORY_MAP[label], \"cropped_images\", model)\n",
    "            ).convert(\"RGB\")\n",
    "\n",
    "        # Randomly sample a negative (ensuring it is not the same as the anchor)\n",
    "        while (negative_index := random.randrange(0, len(self.data))) == index:\n",
    "            pass\n",
    "\n",
    "        negative_model, negative_garment, negative_label = self.data.iloc[\n",
    "            negative_index\n",
    "        ]\n",
    "\n",
    "        # Load the negative image (random choice between model and garment)\n",
    "        if random.choice([True, False]):\n",
    "            negative = Image.open(\n",
    "                os.path.join(\n",
    "                    self.root,\n",
    "                    DIRECTORY_MAP[negative_label],\n",
    "                    \"cropped_images\",\n",
    "                    negative_garment,\n",
    "                )\n",
    "            ).convert(\"RGB\")\n",
    "        else:\n",
    "            negative = Image.open(\n",
    "                os.path.join(\n",
    "                    self.root,\n",
    "                    DIRECTORY_MAP[negative_label],\n",
    "                    \"cropped_images\",\n",
    "                    negative_model,\n",
    "                )\n",
    "            ).convert(\"RGB\")\n",
    "\n",
    "        # Resize & convert to tensors\n",
    "        anchor = self.transforms(anchor)\n",
    "        positive = self.transforms(positive)\n",
    "        negative = self.transforms(negative)\n",
    "\n",
    "        return anchor, positive, negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model: nn.Module,\n",
    "    train_data: DataLoader,\n",
    "    test_data: DataLoader,\n",
    "    loss_fcn: nn.Module,\n",
    "    epochs: int = 10,\n",
    "    device: str = \"cpu\",\n",
    "    log_dir: str = \"./logs\",\n",
    "    output_dir: str = \"./models\",\n",
    "    model_name: str = \"ResNet50\",\n",
    "):\n",
    "    # Create the log & output directories if they don't exist\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_dir, model_name), exist_ok=True)\n",
    "\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    # Wandb logger\n",
    "    logger = wandb.init(dir=log_dir, project=\"fashion-atlas\", name=model_name)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Set model to training mode\n",
    "        model.train()\n",
    "\n",
    "        for i, (anchor, positive, negative) in tqdm(\n",
    "            enumerate(train_data),\n",
    "            f\"Epoch {epoch} Training\",\n",
    "            unit=\"batch\",\n",
    "            total=len(train_data),\n",
    "        ):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Send images to the device\n",
    "            anchor = anchor.to(device)\n",
    "            positive = positive.to(device)\n",
    "            negative = negative.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            anchor_features = model(anchor)\n",
    "            positive_features = model(positive)\n",
    "            negative_features = model(negative)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = loss_fcn(anchor_features, positive_features, negative_features)\n",
    "\n",
    "            # Log loss to tensorboard\n",
    "            logger.log({\"Train/Triplet Loss\": loss, \"Train/Learning Rate\": optimizer.param_groups[-1]['lr']})\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Evaluate the model on the testing data\n",
    "        model.eval()\n",
    "\n",
    "        validation_loss = 0.0\n",
    "        euclidean_distance_ap = 0.0\n",
    "        euclidean_distance_an = 0.0\n",
    "        similarity_ap = 0.0\n",
    "        similarity_an = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, (anchor, positive, negative) in tqdm(\n",
    "                enumerate(test_data),\n",
    "                f\"Epoch {epoch} Evaluation\",\n",
    "                unit=\"batch\",\n",
    "                total=len(test_data),\n",
    "            ):\n",
    "                # Send images to the device\n",
    "                anchor = anchor.to(device)\n",
    "                positive = positive.to(device)\n",
    "                negative = negative.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                anchor_features = model(anchor)\n",
    "                positive_features = model(positive)\n",
    "                negative_features = model(negative)\n",
    "\n",
    "                # Compute the loss\n",
    "                validation_loss += loss_fcn(\n",
    "                    anchor_features, positive_features, negative_features\n",
    "                )\n",
    "\n",
    "                # Compute the Euclidean distance for the positive and negative pairs\n",
    "                euclidean_distance_ap += torch.norm(\n",
    "                    anchor_features - positive_features, dim=1\n",
    "                ).sum()\n",
    "\n",
    "                euclidean_distance_an += torch.norm(\n",
    "                    anchor_features - negative_features, dim=1\n",
    "                ).sum()\n",
    "\n",
    "                # Compute the Cosine similarity for the positive and negative pairs\n",
    "                similarity_ap += torch.cosine_similarity(anchor_features, positive_features).mean()\n",
    "                similarity_an += torch.cosine_similarity(anchor_features, negative_features).mean()\n",
    "\n",
    "        # Log validation metrics\n",
    "        logger.log({\"Val/Triplet Loss\": validation_loss / len(test_data), \"Val/Euclidean Distance Ratio (AN % AP)\": euclidean_distance_an / euclidean_distance_ap, \"Val/Cosine Similarity Ratio (AP % AN)\": similarity_ap / similarity_an})\n",
    "\n",
    "        print(f\"Epoch {epoch} Validation Loss: {validation_loss / len(test_data)}\")\n",
    "\n",
    "        # Save the model\n",
    "        torch.save(model.state_dict(), f\"{os.path.join(output_dir, model_name)}/checkpoint-{epoch + 1}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "train_data = FashionDataset(\"data/DressCode\", \"data/DressCode/train_pairs_cropped.txt\")\n",
    "\n",
    "test_data = FashionDataset(\n",
    "    \"data/DressCode\", \"data/DressCode/test_pairs_paired_cropped.txt\"\n",
    ")\n",
    "\n",
    "\n",
    "# Define the training dataloader\n",
    "train_loader = DataLoader(train_data, batch_size=24, shuffle=True)\n",
    "\n",
    "# Define the validation dataloader\n",
    "test_loader = DataLoader(test_data, batch_size=24, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = models.resnet101()\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtroydutton\u001b[0m (\u001b[33mtroydutton-dev\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./logs\\wandb\\run-20240504_213523-vit3xhh0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/troydutton-dev/fashion-atlas/runs/vit3xhh0/workspace' target=\"_blank\">ResNet101 Cosine Similarity Margin=1</a></strong> to <a href='https://wandb.ai/troydutton-dev/fashion-atlas' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/troydutton-dev/fashion-atlas' target=\"_blank\">https://wandb.ai/troydutton-dev/fashion-atlas</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/troydutton-dev/fashion-atlas/runs/vit3xhh0/workspace' target=\"_blank\">https://wandb.ai/troydutton-dev/fashion-atlas/runs/vit3xhh0/workspace</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 Training: 100%|██████████| 2006/2006 [18:25<00:00,  1.81batch/s]\n",
      "Epoch 0 Evaluation: 100%|██████████| 225/225 [01:12<00:00,  3.10batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Validation Loss: 0.2804301679134369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training: 100%|██████████| 2006/2006 [16:50<00:00,  1.98batch/s]\n",
      "Epoch 1 Evaluation: 100%|██████████| 225/225 [01:07<00:00,  3.34batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Validation Loss: 0.25539010763168335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 Training: 100%|██████████| 2006/2006 [16:37<00:00,  2.01batch/s]\n",
      "Epoch 2 Evaluation: 100%|██████████| 225/225 [01:05<00:00,  3.45batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Validation Loss: 0.2286396026611328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 Training: 100%|██████████| 2006/2006 [16:36<00:00,  2.01batch/s]\n",
      "Epoch 3 Evaluation: 100%|██████████| 225/225 [01:05<00:00,  3.43batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Validation Loss: 0.21030673384666443\n"
     ]
    }
   ],
   "source": [
    "train(\n",
    "    model,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    nn.TripletMarginWithDistanceLoss(\n",
    "        distance_function=lambda x, y: 1 - torch.cosine_similarity(x, y), margin=1\n",
    "    ),\n",
    "    epochs=4,\n",
    "    device=device,\n",
    "    model_name=\"ResNet101 Cosine Similarity Margin=1\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
